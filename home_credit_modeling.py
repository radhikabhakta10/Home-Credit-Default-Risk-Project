# -*- coding: utf-8 -*-
"""home_credit_modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12zxku12bLbctyuFvF3eOHHswH-ifMiUP

NAIVE BAYES
"""

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import MinMaxScaler
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import pandas as pd

# Load dataset
file_path = "/content/application_train.csv"
df = pd.read_csv(file_path)

# Drop high-missing columns
df = df.dropna(thresh=len(df) * 0.6, axis=1)

# Copy the dataset to avoid SettingWithCopyWarning
df = df.copy()

# Fill missing values
for col in df.columns:
    if df[col].dtype == 'object':
        df[col].fillna(df[col].mode()[0], inplace=True)  # Categorical columns
    else:
        df[col].fillna(df[col].median(), inplace=True)  # Numerical columns

# Encode categorical features
df = pd.get_dummies(df, drop_first=True)  # One-hot encoding

# Define features and target
X = df.drop(columns=['TARGET'])
y = df['TARGET']

# Normalize features using MinMaxScaler
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Train Naïve Bayes model
nb_classifier = GaussianNB()
nb_classifier.fit(X_train, y_train)

# Predict and evaluate
y_pred = nb_classifier.predict(X_test)

print(f"Accuracy: {accuracy_score(y_test, y_pred):.2f}")
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

"""This code performs a series of steps to preprocess a dataset and train a Naive Bayes classifier. It starts by loading the dataset and dropping columns with more than 40% missing values. It then fills missing values for categorical columns using the mode (most frequent value) and for numerical columns using the median. The categorical features are one-hot encoded, converting them into binary columns to make them suitable for machine learning models. The features are normalized using Min-Max scaling to bring all numerical variables into a consistent range between 0 and 1. To address class imbalance, the code applies SMOTE (Synthetic Minority Over-sampling Technique) to create synthetic samples for the minority class, ensuring the dataset has a more balanced distribution. The data is split into training and testing sets, with 80% used for training and 20% for testing. A Gaussian Naive Bayes classifier is trained on the resampled data, and predictions are made on the test set. The model's performance is evaluated through accuracy, a classification report (which includes precision, recall, F1-score, and support for each class), and a confusion matrix, providing a detailed assessment of the model's ability to correctly classify the target variable.

The Naive Bayes model achieved an accuracy of 54%. The classification report shows high recall (94%) for defaulters (Class 1), meaning the model is effective at identifying those who default on loans. However, it struggles to correctly classify non-defaulters (Class 0), with only 14% recall, leading to a large number of false positives (48,653 cases). This shows great imbalance within this model. This is also reflected in the confusion matrix, where the model misclassifies most non-defaulters as defaulters while correctly predicting most actual defaulters. The likely causes include class imbalance, feature correlations, and the Naïve Bayes assumption of feature independence, which may not hold for financial data. To improve performance.

RANDOM FOREST
"""

# Install necessary libraries
!pip install pandas scikit-learn imbalanced-learn

# Import required libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Load dataset
file_path = "/content/application_train.csv"  # Update the path if needed
df = pd.read_csv(file_path)

# Drop columns with too many missing values (threshold: 40%)
df = df.dropna(thresh=len(df) * 0.6, axis=1)

# Copy to avoid SettingWithCopyWarning
df = df.copy()

# Fill missing values
for col in df.columns:
    if df[col].dtype == 'object':  # Categorical columns
        df[col].fillna(df[col].mode()[0], inplace=True)
    else:  # Numerical columns
        df[col].fillna(df[col].median(), inplace=True)

# Encode categorical features
df = pd.get_dummies(df, drop_first=True)

# Define features and target
X = df.drop(columns=['TARGET'])  # Features
y = df['TARGET']  # Target variable

# Normalize numerical features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# Initialize and train Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

# Make predictions
y_pred = rf_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

"""The Random Forest model achieved 96% accuracy, demonstrating strong performance in distinguishing between defaulters and non-defaulters. The precision, recall, and F1-scores for both classes are balanced at 0.96, indicating that the model maked reliable predictions. The confusion matrix shows that it correctly identified 56,334 non-defaulters, misclassifying only 202 as defaulters. However, while it correctly classified 52,024 defaulters, it still missed 4,515 cases, meaning some actual defaulters were incorrectly labeled as non-defaulters. This suggests the model slightly favors non-defaulters, which could be addressed by adjusting class weights or fine-tuning hyperparameters. This is not surpsising due to the imbalance in the data and within the reslts of the Naive Bayes. Overall, the model performs exceptionally well, but further refinements could help reduce false negatives and improve recall for defaulters."""

from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt

# Get prediction probabilities for the positive class (class 1)
y_prob = rf_classifier.predict_proba(X_test)[:, 1]

# Calculate the ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_prob)

# Calculate the AUC score
roc_auc = roc_auc_score(y_test, y_prob)

# Print the AUC score
print(f"AUC-ROC: {roc_auc:.2f}")

# Plot the ROC curve
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='b', label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc='lower right')
plt.show()

"""CATBOOST"""

!pip install catboost

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy:.4f}')

# Classification report
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.title('Confusion Matrix')
plt.show()

"""The model achieved an impressive accuracy of 0.9517, indicating strong overall performance. The classification report further breaks down the results for each class. For Class 0, the model has a precision of 0.92, recall of 0.99, and an f1-score of 0.95, showing that it is highly reliable when identifying Class 0 instances, with fewer false positives. For Class 1, the precision is 0.99, recall is 0.92, and the f1-score is 0.95, demonstrating excellent performance in detecting Class 1 instances while maintaining low false negatives. The macro average and weighted average for precision, recall, and f1-score are all 0.95, suggesting that the model is balanced in its ability to predict both classes."""

## Models Considered

### Naive Bayes Classifier


####Drop high-missing columns
`
df = df.dropna(thresh=len(df) * 0.6, axis=1)
`

####Fill missing values
`
for col in df.columns:
    if df[col].dtype == 'object':
        df[col].fillna(df[col].mode()[0], inplace=True)
    else:
        df[col].fillna(df[col].median(), inplace=True)
`

####Encode categorical features
`
df = pd.get_dummies(df, drop_first=True)  # One-hot encoding
`

####Define features and target
`
X = df.drop(columns=['TARGET'])
y = df['TARGET']
`

####Normalize features using MinMaxScaler
`
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)
`

####Handle class imbalance using SMOTE
`
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)
`

####Split into training and testing sets
`
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)
`

####Train Naïve Bayes model
`
nb_classifier = GaussianNB()
nb_classifier.fit(X_train, y_train)
`

####Predict and evaluate
`
y_pred = nb_classifier.predict(X_test)
`

This code performs a series of steps to preprocess a dataset and train a Naive Bayes classifier. It starts by loading the dataset and dropping columns with more than 40% missing values. It then fills missing values for categorical columns using the mode (most frequent value) and for numerical columns using the median. The categorical features are one-hot encoded, converting them into binary columns to make them suitable for machine learning models. The features are normalized using Min-Max scaling to bring all numerical variables into a consistent range between 0 and 1. To address class imbalance, the code applies SMOTE (Synthetic Minority Over-sampling Technique) to create synthetic samples for the minority class, ensuring the dataset has a more balanced distribution. The data is split into training and testing sets, with 80% used for training and 20% for testing. A Gaussian Naive Bayes classifier is trained on the resampled data, and predictions are made on the test set. The model's performance is evaluated through accuracy, a classification report (which includes precision, recall, F1-score, and support for each class), and a confusion matrix, providing a detailed assessment of the model's ability to correctly classify the target variable.

The Naive Bayes model achieved an accuracy of 54%. The classification report shows high recall (94%) for defaulters (Class 1), meaning the model is effective at identifying those who default on loans. However, it struggles to correctly classify non-defaulters (Class 0), with only 14% recall, leading to a large number of false positives (48,653 cases). This shows great imbalance within this model. This is also reflected in the confusion matrix, where the model misclassifies most non-defaulters as defaulters while correctly predicting most actual defaulters. The likely causes include class imbalance, feature correlations, and the Naïve Bayes assumption of feature independence, which may not hold for financial data. To improve performance.